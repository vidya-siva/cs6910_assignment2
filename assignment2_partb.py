# -*- coding: utf-8 -*-
"""assignment2_partB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oQOPZ1uO8JJqDWtKQVG50Cd15fZMOaYW
"""

# import tensorflow as tf
# %tensorflow_version 2.x
import tensorflow as tf
import timeit
import numpy as np
from warmUp import *
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions

def check_cpu_gpu()

  # GPU test
  cpu()
  gpu()

  # Run the op several times.
  print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
        '(batch x height x width x channel). Sum of ten runs.')
  print('CPU (s):')
  cpu_time = timeit.timeit('cpu()', number=10, setup="from __main__ import cpu")
  print(cpu_time)
  print('GPU (s):')
  gpu_time = timeit.timeit('gpu()', number=10, setup="from __main__ import gpu")
  print(gpu_time)
  print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))


def train(config=False):
  # with wandb.init(config=config):
  with tf.device('/device:GPU:0'):

    #Dynamic values for sweeping
    if isSweep:
      config = sweepConfig
      if config:
        print("Running the Model with the dynamic parameters ")
        print(config)
        wandb.init(config=config)
        config = wandb.init().config
        wandb.run.name = 'model_{}_epochs_{}_denseSize_{}_dropOut_{}_batchNorm_{}'\
                          .format(config.modelName, config.num_epochs, config.denseSize,config.dropOut,config.isDataAug)
        modelName  = config.modelName
        denseSize  = config.denseSize
        num_epochs = config.num_epochs
        dropOut    = config.dropOut
        batchNorm  = config.batchNorm
        isDataAug  = config.isDataAug


    if(modelName == "ResNet50"):
      imgHeight = 224
      imgWidth  = 224

      base_model = ResNet50(
        weights = 'imagenet', # loading the weights, pre trained on ImageNet
        input_shape = (imgHeight,imgWidth,3),
        include_top = False # Dense layers of the model are not included at the top
        )

    elif(modelName == "InceptionV3"):
      imgHeight = 299
      imgWidth  = 299

      base_model = InceptionV3(
        weights = 'imagenet', # loading the weights, pre trained on ImageNet
        input_shape = (imgHeight,imgWidth,3),
        include_top = False # Dense layers of the model are not included at the top
        )

    elif(modelName == "Xception"):
      imgHeight = 299
      imgWidth  = 299

      base_model = Xception(
        weights = 'imagenet', # loading the weights, pre trained on ImageNet
        input_shape = (imgHeight,imgWidth,3),
        include_top = False # Dense layers of the model are not included at the top
        )

    elif(modelName == "InceptionResNetV2"):
      imgHeight = 299
      imgWidth  = 299

      base_model = InceptionResNetV2(
        weights = 'imagenet', # loading the weights, pre trained on ImageNet
        input_shape = (imgHeight,imgWidth,3),
        include_top = False # Dense layers of the model are not included at the top
        )


    imgSize = (imgHeight, imgWidth) 
    batchSize = 32 

    if isDataAug == 0 :
      train_data = tf.keras.utils.image_dataset_from_directory(
                            directory = train_dir,
                            labels = 'inferred',  
                            label_mode = 'categorical',
                            color_mode = 'rgb',
                            batch_size = batchSize,
                            image_size = imgSize,
                            shuffle = True,
                            seed = 5,
                            validation_split = 0.2,
                            subset = 'training')

      val_data = tf.keras.utils.image_dataset_from_directory(
                            directory = train_dir,
                            labels = 'inferred',  
                            label_mode = 'categorical',
                            color_mode = 'rgb',
                            batch_size = batchSize,
                            image_size = imgSize,
                            shuffle = True,
                            seed = 5,
                            validation_split = 0.2,
                            subset = 'validation')


      # Retaining 50 percent of train and validation data and discarding the rest
      len_train, len_val = len(train_data), len(val_data)
      train_data = train_data.take(int(0.5*len_train))
      val_data = val_data.take(int(0.5*len_val))

    else:
      train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
                      rescale=1./255,
                      validation_split = 0.2,
                      shear_range=0.2,
                      zoom_range=0.2,
                      featurewise_center=False,  # set input mean to 0 over the dataset
                      samplewise_center=False,  # set each sample mean to 0
                      featurewise_std_normalization=False,  # divide inputs by std of the dataset
                      samplewise_std_normalization=False,  # divide each input by its std
                      zca_whitening=False,  # apply ZCA whitening
                      rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
                      width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
                      height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
                      horizontal_flip=True,  # randomly flip images
                      vertical_flip=False
                      )


      train_data_aug = train_datagen.flow_from_directory(
              train_dir,
              subset='training',
              target_size=imgSize,
              batch_size=batchSize,
              class_mode='categorical',
              shuffle = True,
              seed = 123)

      val_data_aug = train_datagen.flow_from_directory(
              train_dir,
              subset='validation',
              target_size=imgSize,
              batch_size=batchSize,
              class_mode='categorical',
              shuffle = True,
              seed = 123)



      #Hardcoded for testing 
      denseSize = 64
      optimizer = 'adam'
      num_epochs = 50
      isDataAug = 1
      batchNorm=1 
      dropOut =0.4
      
      
    for layers in base_model.layers:
      layers.trainable = False

    model = tf.keras.Sequential([
      tf.keras.Input(shape=(imgHeight,imgWidth,3,)),
      base_model,
      Flatten(),
      Dense(denseSize,activation='relu'),
      ])
      
    if batchNorm == 1:
      model.add(BatchNormalization())

    model.add(Dropout(dropOut))
    model.add(Dense(denseSize, activation='relu'))
    model.add(Dropout(dropOut))
    model.add(Dense(10 ,activation='softmax'))

    model.compile(
        optimizer = 'adam',  # Adam Optimizer
        # Loss function to minimize
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),#'categorical_crossentropy',
        # List of metrics to monitor
        metrics=['accuracy'],
        )

    if isDataAug == 0 :
      hist = model.fit(
          train_data,
          epochs=num_epochs,
          validation_data=val_data,
          callbacks = [wandb.keras.WandbCallback()]
          )
    
    else:
      hist = model.fit(
          train_data_aug,
          epochs=num_epochs,
          validation_data=val_data_aug,
          callbacks = [wandb.keras.WandbCallback()]
          )


