# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18UNtFpiHL84_eQBdNjBynn5kTtK2AoYr
"""

# use this code if you are using google colab
# from google.colab import drive
# drive.mount('/content/drive')


from numpy import unique, argmax
# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow.keras.datasets.mnist import load_data
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, Activation, Rescaling,BatchNormalization
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers.experimental.preprocessing import Rescaling
from tensorflow.keras import layers
import timeit
from matplotlib import pyplot
import matplotlib.pyplot as plt
import numpy as np
import cv2 
import os,sys
import random
import pickle
from PIL import Image
import pathlib
import wandb
sys.path.append(os.path.dirname(os.getcwd()))
from warmUp import *

# We run each op once to warm up; see: https://stackoverflow.com/a/45067900
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error states that this notebook is not '
      'configured to use a GPU. Change this in the notebook Settings via the Runtime type or  '
      'command palette (cmd/ctrl-shift-P) or the Edit menu .\n\n')
  raise SystemError('GPU device not found')
cpu()
gpu()

# Run the op several times.
print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
      '(batch x height x width x channel). Sum of ten runs.')
print('CPU (s):')
cpu_time = timeit.timeit('cpu()', number=10, setup="from __main__ import cpu")
print(cpu_time)
print('GPU (s):')
gpu_time = timeit.timeit('gpu()', number=10, setup="from __main__ import gpu")
print(gpu_time)
print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))

path = os.path.dirname(os.getcwd())
print(path)
train_dir =path + "/inaturalist_12K/train"
val_dir = path + "/inaturalist_12K/val"

imgHeight =  256#@param
imgWidth =  256#@param

imgSize = (imgHeight, imgWidth) 
batchSize = 32 #@param

train_data = tf.keras.utils.image_dataset_from_directory(
                      directory = train_dir,
                      labels = 'inferred',  
                      label_mode = 'categorical',
                      color_mode = 'rgb',
                      batch_size = batchSize,
                      image_size = imgSize,
                      shuffle = True,
                      seed = 5,
                      validation_split = 0.2,
                      subset = 'training')

val_data = tf.keras.utils.image_dataset_from_directory(
                      directory = train_dir,
                      labels = 'inferred',  
                      label_mode = 'categorical',
                      color_mode = 'rgb',
                      batch_size = batchSize,
                      image_size = imgSize,
                      shuffle = True,
                      seed = 5,
                      validation_split = 0.2,
                      subset = 'validation')


# Retaining 10 percent of train and validation data and discarding the rest
len_train, len_val = len(train_data), len(val_data)
train_data = train_data.take(int(0.25*len_train))
val_data = val_data.take(int(0.25*len_val))


train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
                rescale=1./255,
                validation_split = 0.2,
                shear_range=0.2,
                zoom_range=0.2,
                featurewise_center=False,  # set input mean to 0 over the dataset
                samplewise_center=False,  # set each sample mean to 0
                featurewise_std_normalization=False,  # divide inputs by std of the dataset
                samplewise_std_normalization=False,  # divide each input by its std
                zca_whitening=False,  # apply ZCA whitening
                rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
                horizontal_flip=True,  # randomly flip images
                vertical_flip=False
                )


train_data_aug = train_datagen.flow_from_directory(
        train_dir,
        subset='training',
        target_size=imgSize,
        batch_size=batchSize,
        class_mode='categorical',
        shuffle = True,
        seed = 123)

val_data_aug = train_datagen.flow_from_directory(
        train_dir,
        subset='validation',
        target_size=imgSize,
        batch_size=batchSize,
        class_mode='categorical',
        shuffle = True,
        seed = 123)

#Setting it to false to enable testing , Run the next cell to enable it and disable testing. 
isSweep = False


print('Train data and Val Data has been set, Now lets use GPU and Define our Model')

def train(config=False):
  with tf.device('/device:GPU:0'):

    #Hardcoded for testing 
    filterOrganisation = 0.5 
    firstLayerFliterCount = 64 
    convLayers = 5 
    activation = 'relu'
    denseSize = 32 
    optimizer = 'adam'
    num_epochs = 50
    kernelSize = 3
    isDataAug = 0
    batchNorm=1 
    dropOut =0.2
    if isSweep:
      config = sweepConfig
      if config:
        print("Running the Model with the dynamic parameters ")
        print(config)
        wandb.init(config=config)
        config = wandb.init().config
        wandb.run.name = 'firstLayerFliterCount_{}_filter_org_{}_kernelSize_{}_denseSize_{}'\
                          .format(config.firstLayerFliterCount, config.filterOrganisation, config.kernelSize,config.denseSize)
        filterOrganisation = config.filterOrganisation
        firstLayerFliterCount = config.firstLayerFliterCount
        convLayers = config.convLayers
        activation = config.activation
        denseSize = config.denseSize
        optimizer = config.optimizer
        num_epochs = config.num_epochs
        kernelSize = config.kernelSize
        isDataAug = config.isDataAug
        dropOut = config.dropOut


    filterSize = [int(firstLayerFliterCount*(filterOrganisation**layer_num)) for layer_num in range(convLayers)]

    model = Sequential()

    # Applying some convolution and pooling layers
    for layer_num in range(convLayers):
        model.add(Conv2D(filterSize[layer_num], (kernelSize, kernelSize), input_shape=(imgHeight,imgWidth,3)))
        #Normalisation
        if batchNorm:
          model.add(BatchNormalization(axis = -1))
        model.add(Activation('relu'))
        model.add(MaxPool2D(pool_size=(2, 2)))           


    #dense layer
    model.add(Flatten())
    model.add(Dense(denseSize,activation='relu'))

    #Normalisation
    if batchNorm:
      model.add(BatchNormalization(axis = -1))

    #Dropout 
    if dropOut > 0:
      model.add(Dropout(rate = dropOut))

    #output layer
    model.add(Dense(10,activation='softmax'))

    print(model.summary())

    model.compile(
        optimizer=optimizer,  # Optimizer
        # Loss function to minimize
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),#'categorical_crossentropy',
        # List of metrics to monitor
        metrics=['accuracy'],
        )
    
    final_train_data =  train_data_aug if isDataAug else train_data
    final_val_data =  val_data_aug if isDataAug else val_data

    #Hardcoded steps to 15 in order to speed up the training , but this leads to lesser accuracy. use len(train_data_aug) or train_data_aug.samples for maximum accuracy but it is expensive
    esteps =  15 if isDataAug else len(train_data)
    valSteps = 15 if isDataAug else len(val_data)

    if isSweep:
      history = model.fit(
                        final_train_data,
                        steps_per_epoch = esteps ,
                        validation_data = final_val_data, 
                        validation_steps = valSteps,
                        epochs = num_epochs,
                        callbacks = [wandb.keras.WandbCallback()]
                        )
    else:
      history = model.fit(
                      final_train_data,
                      steps_per_epoch = esteps ,
                      validation_data = final_val_data, 
                      validation_steps = valSteps,
                      epochs = num_epochs
                      )
    #Saving the Model for Future use.   
    model.save(pathlib.Path('DLAssignment2PartAModel'))

train()
# #Trained successfully :)

